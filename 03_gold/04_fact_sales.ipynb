{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ae9d93-aaba-4390-93d8-1447717c5dd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-----------------------------\n",
    "-- CREATE FACT SALES TABLE\n",
    "-----------------------------\n",
    "CREATE TABLE IF NOT EXISTS real_time_projects.ecommerce_historical.fact_sales (\n",
    "    order_id STRING,\n",
    "    order_item_id STRING,\n",
    "    customer_id STRING,\n",
    "    product_id STRING,\n",
    "\n",
    "    order_purchase_timestamp TIMESTAMP,\n",
    "    order_date DATE,\n",
    "\n",
    "    price DOUBLE,\n",
    "    freight_value DOUBLE,\n",
    "    revenue DOUBLE,\n",
    "\n",
    "    customer_state STRING,\n",
    "    product_category_name STRING,\n",
    "\n",
    "    payment_value DOUBLE,\n",
    "    order_status STRING,\n",
    "    load_date DATE\n",
    ")\n",
    "USING DELTA;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81b3e662-5a77-4a94-8c45-d0bb22b89269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# --------------------\n",
    "# READ DELTA TABLE\n",
    "# --------------------\n",
    "orders_df = spark.table(\"real_time_projects.ecommerce_historical.orders\")\n",
    "order_items_df = spark.table(\"real_time_projects.ecommerce_historical.order_items\")\n",
    "payments_df = spark.table(\"real_time_projects.ecommerce_historical.order_payments\")\n",
    "\n",
    "# -------------------------------\n",
    "# READ DIMENSION DELTA TABLE\n",
    "# -------------------------------\n",
    "dim_customers_df = spark.table(\"real_time_projects.ecommerce_historical.dim_customer\").filter(\"is_active = 'Y'\")\n",
    "dim_products_df = spark.table(\"real_time_projects.ecommerce_historical.dim_product\").filter(\"is_active = 'Y'\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# JOIN ORDER & ORDER ITEMS, ENSURE UNIQUE COLUMN NAMES\n",
    "# ---------------------------------------------------------\n",
    "order_items_df = order_items_df.select(\n",
    "    \"order_id\",\n",
    "    col(\"order_item_id\").alias(\"order_item_id\"),\n",
    "    \"product_id\",\n",
    "    \"price\",\n",
    "    \"freight_value\"\n",
    ")\n",
    "\n",
    "# ------------------------------\n",
    "# JOIN ORDER & ORDER ITEMS\n",
    "# ------------------------------\n",
    "sales_df = (\n",
    "    orders_df\n",
    "    .join(\n",
    "        order_items_df,\n",
    "        \"order_id\",\n",
    "        \"inner\"\n",
    "    ) \n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# JOIN CUSTOMER & PRODUCT DIMENSION\n",
    "# ---------------------------------------\n",
    "sales_df = (\n",
    "    sales_df\n",
    "    .join(dim_customers_df, \"customer_id\", \"left\")\n",
    "    .join(dim_products_df, \"product_id\", \"left\")\n",
    ")\n",
    "\n",
    "# -----------------\n",
    "# JOIN PAYMENTS\n",
    "# -----------------\n",
    "sales_df = sales_df.join(\n",
    "    payments_df.groupBy(\"order_id\")\n",
    "        .agg(sum(\"payment_value\").alias(\"payment_value\")),\n",
    "    \"order_id\",\n",
    "    \"left\"\n",
    ")\n",
    "\n",
    "# sales_df.printSchema()\n",
    "# --------------------------\n",
    "# DERIVE FACT TABLE\n",
    "# --------------------------\n",
    "fact_sales_df = (\n",
    "    sales_df\n",
    "    .select(\n",
    "        col(\"order_id\"),\n",
    "        col(\"order_item_id\"),\n",
    "        col(\"customer_id\"),\n",
    "        col(\"product_id\"),\n",
    "\n",
    "        col(\"order_purchase_timestamp\"),\n",
    "        to_date(\"order_purchase_timestamp\").alias(\"order_date\"),\n",
    "\n",
    "        col(\"price\"),\n",
    "        col(\"freight_value\"),\n",
    "        (col(\"price\") + col(\"freight_value\")).alias(\"revenue\"),\n",
    "\n",
    "        col(\"customer_state\"),\n",
    "        col(\"product_category_name\"),\n",
    "\n",
    "        col(\"payment_value\"),\n",
    "        col(\"order_status\"),\n",
    "\n",
    "        current_date().alias(\"load_date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# fact_sales_df.printSchema()\n",
    "\n",
    "# ------------------------\n",
    "# WRITE TO DELTA TABLE\n",
    "# ------------------------\n",
    "fact_sales_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"real_time_projects.ecommerce_historical.fact_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aacebcf-da14-4ff6-8014-b94d6f71d92b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum, when, lit\n",
    "\n",
    "df = spark.table(\"real_time_projects.ecommerce_historical.fact_sales\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# DISPLAY THE NUMBER OF NULL VALUES IN EACH COLUMN OF THE DATAFRAME\n",
    "# ---------------------------------------------------------------------\n",
    "display(\n",
    "    df.select([\n",
    "        count(\n",
    "            when(\n",
    "                col(c).isNull(),\n",
    "                c\n",
    "            )\n",
    "        ).alias(c)\n",
    "        for c in df.columns\n",
    "    ])\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8988713538944435,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04_fact_sales",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
